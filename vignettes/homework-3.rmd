---
title: "Homework 3"
author: "Yingnan Lyu"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The lasso regression vignette}
%\VignetteEncoding{UTF-8}
\usepackage[utf8]{Matrix}
-->

## Homework 3

This vignette answers questions in homework 3 of BIS 557. 

**1.** CASL page 117, question 7. 

A function is implemented to calculate the Epanechnikov kernel and another is implemented to calculate the kernel density based on the Epanechnikov kernel. 

```{r}
# A function that calculate the Epanechnikov kernel
kernel_epan <- function(x, h = 1) {
  x <- x / h
  ran <- as.numeric(abs(x) <= 1)
  val <- (1/h) * (3/4) * (1 - x^2) * ran
  return(val)
}

# A function that calculate the kernel density
# Inputs: x, a training vector; h, bandwidth; x_new, a test set
# Output: the kernel density estimate from the Epanchnikov kernel
kern_density <- function(x, h, x_new) {
  sapply(x_new, function(xi) {
    kh <- kernel_epan((x - xi), h = h)
    f <- sum(kh)/length(x)
    f
  })
}
```

Then generate a set of x from uniform distribution U(0,1). Use bandwidth 0.05, 0.5 and 1 respectively and plot the kernel density against x. The density of U(0,1) is overlaid to each plot using a red dashed line. 

```{r}
# Test the function with hand constructed datasets and bandwidths
set.seed(100)
n <- 100
x <- seq(0, 1, length.out = n)
x_new <- runif(n)
h <- c(0.05, 0.5, 1)

sapply(h, function(hi) {
  f_new <- kern_density(x, hi, sort(x_new))
  plot(sort(x_new), f_new, type="l",
       main=paste("bandwidth =", hi), xlab="x_new", ylab="Kernel Density")
  curve(dunif(x), col='red', lty=2, add=TRUE)
})
```

Run another test using a set of x from normal distribution N(0,1). Use bandwidth 0.05, 0.5 and 1 respectively and plot the kernel density against x. The density of N(0,1) is overlaid to each plot using a red dashed line. 

```{r}
# Test the function with hand constructed datasets and bandwidths
set.seed(100)
n <- 100
# x <- seq(0, 1, length.out = n)
x <- rnorm(n)
x_new <- rnorm(n)
h <- c(0.05, 0.5, 1)

sapply(h, function(hi) {
  f_new <- kern_density(x, hi, sort(x_new))
  plot(sort(x_new), f_new, type="l",
       main=paste("bandwidth =", hi), xlab="x_new", ylab="Kernel Density")
  curve(dnorm(x), col='red', lty=2, add=TRUE)
})
```

**2.** CASL page 200, question 3

If f is a convex function, we have the property 
$$f(tb_1 + (1-t)b_2) \le tf(b_1) + (1-t)f(b_2).$$

Similarly if g is a convex function, we have
$$g(tb_1 + (1-t)b_2) \le tg(b_1) + (1-t)g(b_2).$$

Suppose h(x) = f(x) + g(x), then 
$$
\begin{aligned}
&h(tb_1 + (1-t)b_2) = f(tb_1 + (1-t)b_2) + g(tb_1 + (1-t)b_2)\\
&\le  tf(b_1) + (1-t)f(b_2) + tg(b_1) + (1-t)g(b_2) = th(b_1) + (1-t)h(b_2).
\end{aligned}
$$

So the sum of f and g is a convex function as well.

**3.** CASL page 200, question 4

First show the absolute value function is convex. 

Suppose f(x) = |x|, then 

$$
\begin{aligned}
f(tb_1 + (1-t)b_2) &= |tb_1 + (1-t)b_2|\\ 
&\le |tb_1| + |(1-t)b_2| \text{ by triangle inequality}\\ 
&= t|b_1| + (1-t)|b_2| \text{ as 0<t<1}\\
&= tf(b_1) + (1-t)f(b_2)
\end{aligned}
$$

So absolute value function f is a convex function. 

According to the result from the last exercise, the sum of convex functions is also convex. We know an absolute value function |b_j| is convex, so l1-norm $||b||_1 = \sum_j |b_j|$, as the sum of a series of $|b_j|$, is convex as well. 

**4.** CASL page 200, question 5

The elastic objective function is 

$$
f(b;\lambda, \alpha) = \frac{1}{2n} ||y-Xb||^2_2 + \lambda \left((1-\alpha) \frac{1}{2} ||b||_2^2 + \alpha||b||_1\right)
$$

As $(y_i-X_{ij}b_j)^2$ is a convex function, then the sum $||y-Xb||^2_2 = \sum_i \sum_j (y_i-X_{ij}b_j)^2$ is convex function as well, as proven in qusetion 2. 

Similarly, as $(b_j)^2$ is convex, then the sum $||b||_2^2 = \sum_j (b_j)^2$ is convex too.

Moreover, it has already been proven in question 3 that $||b||_1$ is a convex function as well. 

According to the definition of elastic net, n>0, $\lambda > 0$ and $\alpha \in [0,1]$, so the sum of the three parts of convex function 
$$
\frac{1}{2n} ||y-Xb||^2_2 + \lambda \left((1-\alpha) \frac{1}{2} ||b||_2^2 + \alpha||b||_1\right)
$$
is a convex function. 

**5.** CASL page 200, question 6

First, make a dataset. 

```{r}
library(bis557)
library(Matrix)
library(glmnet)

# The dataset
set.seed(1)
n <- 1000L
p <- 5000L
X <- matrix(rnorm(n * p), ncol = p)
beta <- c(seq(1, 0.1, length.out=(10L)), rep(0, p - 10L))
y <- X %*% beta + rnorm(n = n, sd = 0.15)
```

Then use glmnet to fit a lasso regression and check the KKT conditions for betas corresponding to the best lambda value (the lambda that minimize the mean squared error).

```{r}
# glmnet
gfit <- cv.glmnet(X, y, alpha=1)
lambda.min <- gfit$lambda.min
beta_hat <- gfit$glmnet.fit$beta[,which(gfit$lambda == gfit$lambda.min)]

# Check KKT for beta from glmnet
lasso_reg_with_screening <- function(X, y, b, lambda) {
  resids <- y - X %*% b
  s <- apply(X, 2, function(xj) crossprod(xj, resids)) / lambda / nrow(X)
  (b == 0) & (abs(s) >= 1)
}

kkt.violat.glmnet <- lasso_reg_with_screening(X,y,beta_hat, lambda.min)
sum(kkt.violat.glmnet)/length(kkt.violat.glmnet)
```

Only 0.22% of beta returned by glmnet violated KKT condition. The betas from glmnet are relatively valid.  
