# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1)) {N1_state[i]=1}
if (identical(path[(k-1):k], N2)) {N2_state[i]=1}
}
}
length(N1_state)
head(N1_state)
sum(N1_state)
path
npath <- 10000
ntoss <- 1000
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1)) {N1_state[i]=N1_state[i]+1}
if (identical(path[(k-1):k], N2)) {N2_state[i]=N2_state[i]+1}
}
}
npath <- 10000
ntoss <- 1000
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1)) {
N1_state[i]=k
break}
if (identical(path[(k-1):k], N2)) {
N2_state[i]=k
break}
}
}
head(N1_state)
tail(N1_state)
table(N1_state)
table(N2_state)
mean(N1_state)
mean(N2_state)
npath <- 10000
ntoss <- 20
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1)) {
N1_state[i]=k
break}
if (identical(path[(k-1):k], N2)) {
N2_state[i]=k
break}
}
}
table(N1_state)
table(N2_state)
mean(N1_state)
mean(N2_state)
npath <- 10000
ntoss <- 100
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1)) {
N1_state[i]=k
break}
if (identical(path[(k-1):k], N2)) {
N2_state[i]=k
break}
}
}
mean(N1_state)
mean(N2_state)
# A function that calculate the Epanechnikov kernel
kernel_epan <- function(x, h = 1) {
x <- x / h
ran <- as.numeric(abs(x) <= 1)
val <- (3/4) * (1 - x^2) * ran
return(val)
}
# A function that calculate the kernel density
# Inputs: x, a training vector; h, bandwidth; x_new, a test set
# Output: the kernel density estimate from the Epanchnikov kernel
kern_density <- function(x, h, x_new) {
sapply(x_new, function(v) {
w <- kernel_epan((v - x), h = h)
f <- sum(w)/length(x)
f
})
}
# Test the function with hand constructed datasets and bandwidths
x <- seq(0, 1, length.out = 100)
h <- 1
x_new <- runif(100)
f_new <- kern_density(x, h, x_new)
plot(x_new, f)
# A function that calculate the Epanechnikov kernel
kernel_epan <- function(x, h = 1) {
x <- x / h
ran <- as.numeric(abs(x) <= 1)
val <- (3/4) * (1 - x^2) * ran
return(val)
}
# A function that calculate the kernel density
# Inputs: x, a training vector; h, bandwidth; x_new, a test set
# Output: the kernel density estimate from the Epanchnikov kernel
kern_density <- function(x, h, x_new) {
sapply(x_new, function(v) {
w <- kernel_epan((v - x), h = h)
f <- sum(w)/length(x)
f
})
}
# Test the function with hand constructed datasets and bandwidths
x <- seq(0, 1, length.out = 100)
h <- 1
x_new <- runif(100)
f_new <- kern_density(x, h, x_new)
plot(x_new, f_new)
pnorm(0.25/3)
qnorm(0.25/3)
qnorm(0.025/3)
log(6.14)
log(.614)
0.85*1.2
0.85*ã€‚8
0.85*.8
npath <- 10000
ntoss <- 100
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1)) {
N1_state[i]=k
break}
if (identical(path[(k-1):k], N2)) {
N2_state[i]=k
break}
}
}
mean(N1_state)
mean(N2_state)
npath <- 10000
ntoss <- 1000
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1)) {
N1_state[i]=k
break}
if (identical(path[(k-1):k], N2)) {
N2_state[i]=k
break}
}
}
mean(N1_state)
mean(N2_state)
npath <- 10000
ntoss <- 100
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1)) {
N1_state[i]=k
break}
if (identical(path[(k-1):k], N2)) {
N2_state[i]=k
break}
}
}
mean(N1_state)
mean(N2_state)
npath <- 10000
ntoss <- 10000
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
set.seed(1)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1)) {
N1_state[i]=k
break}
if (identical(path[(k-1):k], N2)) {
N2_state[i]=k
break}
}
}
npath <- 10000
ntoss <- 1000
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
set.seed(1)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1)) {
N1_state[i]=k
break}
if (identical(path[(k-1):k], N2)) {
N2_state[i]=k
break}
}
}
mean(N1_state)
mean(N2_state)
head(path)
table(N1_state)
qnorm(-0.5)
pnorm(-0.5)
pnorm(0.9)
qnorm(0.9)
library(MASS)
#'
#' ## Metropolis: Random walk Metropolis method
#'
#' ### Show method first in an easy example: N(0,1)
#'
#' We don't need MCMC to simulate from N(0,1) but we'll see how it works for this example first.
#'
# Normal distribution example -----
f1 <- function(x) {1/sqrt(2*pi) * exp(-x^2/2)}
f <- f1
set.seed(181101)
nit <- 100000
path <- numeric(nit)
head(path)
numeric(nit)
initial <- 10
state <- initial
path[1] <- initial
for(i in 2:nit){  # <-- the Metropolis iteration
candidate <- runif(1,state-1,state+1)
ratio <- f(candidate)/f(state)
if(runif(1) < ratio) state <- candidate
path[i] <- state
}
plot(path)
head(state)
truehist(path)
truehist(path[-(1:100)])
curve(f(x), lwd=2, add=T, col=2)
path <- path[-(1:100)]
mean(abs(path) < 1)
mean(abs(path) < 2)
mean(abs(path) < 3)
2*(pnorm(c(1,2,3))-0.5)
# More complicated density -----
# From notes:
f2 <- function(x){
(x > 0) * (x < 3) * (10*x - 3*x^2 ) * sin(x-2)^2 / sqrt(1 + exp(tan(x)))
}
plot(f2, 0, 3)
# Something I made up today:
f3 <- function(x){
(x > 0) * (x < 3) * x * (3-x) * (1.7 + sin(5*x)) * exp(-x/3) # change to x^2
}
plot(f3,0,3)
# Use exactly the same commands as above to simulate from this density:
f <- f3
set.seed(181101)
nit <- 100000
path <- numeric(nit)
initial <- 2
state <- initial
path[1] <- initial
for(i in 2:nit){
candidate <- runif(1,state-1,state+1)
ratio <- f(candidate)/f(state)
if(runif(1) < ratio) state <- candidate
path[i] <- state
}
plot(path[1:2000])
truehist(path)
curve(f, 0, 3, add=T, col=2)
truehist(path)
curve(f(x), 0, 3)
#' Next we'll do a numerical integral to find the normalizing constant to make the function into a density.
truehist(path)
con <- integrate(f, 0, 3)$value
con
?integrate
f1 <- function(x){f(x)/con}
plot(f1, 0, 3, n=1000, add=T, lwd=2, col=2)
knitr::include_graphics('~/Documents/Master2-2/S&DS 538-Probability and Statistics/Homework/ps6_1a.png')
knitr::include_graphics('~/Documents/Master2-2/S&DS 538-Probability and Statistics/Homework/ps6_1ab.png')
knitr::include_graphics('~/Documents/Master2-2/S&DS 538-Probability and Statistics/Homework/ps6_2ab.png')
knitr::include_graphics('~/Documents/Master2-2/S&DS 538-Probability and Statistics/Homework/ps6_3ab.png')
knitr::include_graphics('~/Documents/Master2-2/S&DS 538-Probability and Statistics/Homework/ps6_4ab.png')
identical(c(TRUE, TRUE), c(TRUE, FALSE))
npath <- 10000
ntoss <- 1000
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
# Simulate the path
path <- rep(0, ntoss)
length(path)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
table(path)
npath <- 10000
ntoss <- 50
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
set.seed(1)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1) & N1_state[i] = 0) {
npath <- 10000
ntoss <- 50
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
set.seed(1)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (identical(path[(k-1):k], N1) & N1_state[i] == 0) {
N1_state[i]=k}
if (identical(path[(k-1):k], N2) & N2_state[i] == 0) {
N2_state[i]=k}
}
}
mean(N1_state)
mean(N2_state)
head(N1_state)
npath <- 10000
ntoss <- 50
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
set.seed(1)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (N1_state[i] == 0 & identical(path[(k-1):k], N1)) {
N1_state[i]=k}
else if (N2_state[i] == 0 & identical(path[(k-1):k], N2)) {
N2_state[i]=k}
}
}
mean(N1_state)
mean(N2_state)
npath <- 10000
ntoss <- 100
N1 <- c("H", "H")
N2 <- c("H", "T")
N1_state <- rep(0, npath)
N2_state <- rep(0, npath)
set.seed(1)
for (i in 1:npath) {
# Simulate the path
path <- rep(0, ntoss)
for (j in 1:ntoss) {
path[j] <- sample(c("H", "T"), 1, prob = c(0.5, 0.5))
}
# Find N1 and N2
for (k in 2:ntoss) {
if (N1_state[i] == 0 & identical(path[(k-1):k], N1)) {
N1_state[i]=k}
else if (N2_state[i] == 0 & identical(path[(k-1):k], N2)) {
N2_state[i]=k}
}
}
mean(N1_state)
mean(N2_state)
knitr::include_graphics('~/Documents/Master2-2/S&DS 538-Probability and Statistics/Homework/ps6_4cd.png')
knitr::include_graphics('~/Documents/Master2-2/S&DS 538-Probability and Statistics/Homework/ps6_4ab.png')
knitr::include_graphics('~/Documents/Master2-2/S&DS 538-Probability and Statistics/Homework/ps6_4cd.png')
any(c(1,0,0))
any(c(T,F,F))
any(c(F,F,F))
c(T,F,F) | c(T, F, T)
c(T,F,F) & c(T, F, T)
n <- 1000L
p <- 5000L
X <- matrix(rnorm(n * p), ncol = p)
beta <- c(seq(1, 0.1, length.out=(10L)), rep(0, p - 10L))
y <- X %*% beta + rnorm(n = n, sd = 0.15)
attributes(X)
X <- scale(X)
attributes(X)
ax <- attributes(X)
ax$`scaled:center`
dim(X)
length(ax$`scaled:center`)
dim(b)
?cv.glmnet
library(glmnet)
?cv.glmnet
?glmnet
library(cleanNLP)
load("amazon-reviews-y2k.rda")
setwd("~/Documents/Master2-2/BIS 557-Computational Statistics/bis557")
load("amazon-reviews-y2k.rda")
data("amazon-reviews-y2k.rda")
data(amazon-reviews-y2k)
load("~/Documents/Master2-2/BIS 557-Computational Statistics/bis557/data/amazon-reviews-y2k.rda")
data("amazon-reviews-y2k")
library(bis557)
data("amazon-reviews-y2k")
data("ridge_test")
qnorm(1-0.167/2)
qnorm(1-0.0167/2)
0.05/6
qnorm(1-0.0083/2)
qnorm(0.9)
(2.64 + 1.28)^2*2*6.7^2/1.5^2
614*3
614/0.75
819*3
